{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.model_selection as skl\n",
    "from sklearn import metrics as skm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all features and all labels\n",
    "\n",
    "features =  pd.read_csv(\"data/s-2_features_preprocessed_19_20.csv\") # all features on variable x - used for training\n",
    "all_thresholds = pd.read_csv(\"data/stress_incidence_labels_25.csv\") # labels for 25 thresholds (5-30)\n",
    "\n",
    "# Single column removal with INPLACE - from features\n",
    "features.drop(features.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Single column removal with INPLACE - from labels\n",
    "all_thresholds.drop(all_thresholds.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = list(features.columns)\n",
    "threshold_list = list(all_thresholds.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the features and thresholds dataframes for resampling\n",
    "data = pd.concat([features, all_thresholds], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as svc\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.ensemble import AdaBoostClassifier as adaboost\n",
    "from sklearn.ensemble import GradientBoostingClassifier as gboost\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier as trees\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as logreg\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB as gnb\n",
    "from sklearn.naive_bayes import MultinomialNB as mnb\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier as nn\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as qda\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models and parameters to be tested in GridSearchCV\n",
    "\n",
    "model_params = {\n",
    "    'svm': {\n",
    "        'model': svc(random_state=4),\n",
    "        'params' : {\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "            'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "            'kernel': ['rbf','sigmoid', 'linear']\n",
    "        }  \n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': rf(random_state=4),\n",
    "        'params' : {\n",
    "            'n_estimators': list(range(100,510,100)),\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'min_samples_split':[2,4,6,8]\n",
    "        }\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'model': trees(random_state=4),\n",
    "        'params': {\n",
    "            'criterion': ['gini','entropy'],\n",
    "            'splitter':['best','random'],\n",
    "            'min_samples_split':[2,3,4,5,8,10]\n",
    "        }\n",
    "    },\n",
    "    'ada_boost': {\n",
    "        'model': adaboost(random_state=4),\n",
    "        'params': {\n",
    "            'n_estimators': [20,50,100,150],\n",
    "            'learning_rate': [0.01,0.1,1.0,1.5,2.0]\n",
    "        }\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'model': gboost(random_state=4),\n",
    "        'params': {\n",
    "            'learning_rate':[0.01,0.1,1.0,1.5,2.0],\n",
    "            'n_estimators':[20,50,100,150],\n",
    "            'criterion':['friedman_mse', 'mse']\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression' : {\n",
    "        'model': logreg(solver='liblinear',multi_class='auto'),\n",
    "        'params': {\n",
    "            'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "            'max_iter':[50,75,100,200,300,400,500]\n",
    "        }\n",
    "    },\n",
    "    'naive_bayes_gaussian': {\n",
    "        'model': gnb(),\n",
    "        'params': {\n",
    "            'var_smoothing': [0.000000001,0.0000001,0.00001,0.001,0.1]\n",
    "        }\n",
    "    },\n",
    "    'naive_bayes_multinomial': {\n",
    "        'model': mnb(),\n",
    "        'params': {\n",
    "            'alpha': [0.001,0.01,0.1,1.0,10.,100.,1000.],\n",
    "            'fit_prior': [True,False]\n",
    "        }\n",
    "    },\n",
    "    'k_nearest_neighbors': {\n",
    "        'model':knn(),\n",
    "        'params': {\n",
    "            'n_neighbors':[1,2,5,8,10,15,20],\n",
    "            'p':[1,2,3,4,5],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            'n_jobs': [-2,-1,1]\n",
    "        }\n",
    "    },\n",
    "    'mlp': {\n",
    "        'model':nn(random_state=4),\n",
    "        'params': {\n",
    "            'hidden_layer_sizes':[50,100,150],\n",
    "            'activation': ['identity','logistic','tanh','relu'],\n",
    "            'solver': ['lbfgs', 'sgd','adam'],\n",
    "            'alpha': [0.001,0.01,0.1],\n",
    "            'learning_rate': ['constant','invscaling','adaptive'],\n",
    "            'max_iter': [400,500,600,700,800,900,1000]\n",
    "        }\n",
    "    },\n",
    "    'linear_discriminant': {\n",
    "        'model': lda(),\n",
    "        'params': {\n",
    "            'solver': ['lsqr','eigen'], #'svd' was excluded, as it did not compute the covariance matrix\n",
    "            'shrinkage': [None,'auto',0.01,0.1,0.2,0.5,0.75,1],\n",
    "            'store_covariance':[True,False]\n",
    "        }\n",
    "    },\n",
    "    'quadratic_discriminant': {\n",
    "        'model': qda(),\n",
    "        'params': {\n",
    "            'reg_param': [0.0,0.01,0.1,0.5],\n",
    "            'store_covariance':[True,False],\n",
    "            'tol': [0.001]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a first test with GridSearchCV without searching all possible incidence thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Random Search for thresholds 5-30 using the grid of classifiers and hyperparameters selected in the previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info concerning data splitting into train and test\n",
    "\n",
    "All estimators in scikit where name ends with CV perform cross-validation. You need to keep a separate test set for measuring the performance.\n",
    "\n",
    "So you need to split your whole data to train and test. Forget about this test data for a while.\n",
    "\n",
    "And then pass this train data only to grid-search. GridSearch will split this train data further into train and test to tune the hyper-parameters passed to it. And finally fit the model on the whole train data with best found parameters.\n",
    "\n",
    "Now you need to test this model on the test data you kept aside in the beginning. This will give you the near real world performance of model.\n",
    "\n",
    "If you use the whole data into GridSearchCV, then there would be leakage of test data into parameter tuning and then the final model may not perform that well on newer unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create an empty dictionary with lists to store the outputs of each iteration\n",
    "\n",
    "storing_dict = {'incidence_threshold':[],\n",
    "                'model_name':[],\n",
    "                'auc_score': [],\n",
    "                'sensitivity':[],\n",
    "                'specificity':[],\n",
    "                'f1_score':[],\n",
    "                'avg_precision':[],\n",
    "                'best_parameters':[]\n",
    "               }\n",
    "\n",
    "# iterate for every threshold we want to test\n",
    "for i in threshold_list:\n",
    "#     for each threshold iteration redefine the features (X) and label (y(incidence_threshold))\n",
    "    X = data[feature_list]\n",
    "    y = data[i]\n",
    "    \n",
    "#     augment dataset using SMOTE\n",
    "    sm = SMOTE(random_state=4)\n",
    "    X_sm, y_sm = sm.fit_resample(X, y)\n",
    "    \n",
    "#     split the oversampled dataset into training and test sets\n",
    "    X_train, X_test, y_train, y_test = skl.train_test_split(X_sm,y_sm,test_size=0.30,random_state=4)\n",
    "    \n",
    "#     instantiate the grid and loop for all models we want to test\n",
    "    for k in model_params:\n",
    "        rndm_grid = RandomizedSearchCV(model_params[k]['model'],\n",
    "                    param_distributions=model_params[k]['params'],\n",
    "                    n_iter=100,\n",
    "                    cv=10,\n",
    "                    scoring='roc_auc',\n",
    "                    random_state=4\n",
    "                   )\n",
    "        \n",
    "#         fit the model to the data\n",
    "        clf = rndm_grid.fit(X_train,y_train)\n",
    "        \n",
    "#         apply predictions on the validation set\n",
    "        y_pred = clf.predict(X_test)\n",
    "    \n",
    "#     create a confusion matrix\n",
    "        confusion = skm.confusion_matrix(y_test,y_pred)\n",
    "    \n",
    "#         calculate true positives, true negatives, false positives and false negatives based on the confusion matrix\n",
    "        TP = confusion[1,1]\n",
    "        TN = confusion[0,0]\n",
    "        FP = confusion[0,1]\n",
    "        FN = confusion[1,0]\n",
    "#         calculate additional metrics for reference\n",
    "        f1 = skm.f1_score(y_test,y_pred)\n",
    "        sens = skm.recall_score(y_test, y_pred)\n",
    "        spec = TN / float(TN + FP)\n",
    "        avg_precision = skm.average_precision_score(y_test,y_pred)\n",
    "    \n",
    "#         store the best values for each iteration\n",
    "        storing_dict['incidence_threshold'].append(i)\n",
    "        storing_dict['model_name'].append(k)\n",
    "        storing_dict['auc_score'].append(rndm_grid.best_score_)\n",
    "        storing_dict['sensitivity'].append(sens)\n",
    "        storing_dict['specificity'].append(spec)\n",
    "        storing_dict['f1_score'].append(f1)\n",
    "        storing_dict['avg_precision'].append(avg_precision)\n",
    "        storing_dict['best_parameters'].append(rndm_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in storing_list_test:\n",
    "    print(i['auc_score'], '--->', i['model_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the results and find the best performing model for this testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the dictionary with the stored values to a dataframe and inspect it\n",
    "df_results = pd.DataFrame(storing_dict)\n",
    "df_results.sort_values('auc_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the dataframe to an excel file\n",
    "to_extract = df_results.sort_values('auc_score', ascending=False)\n",
    "with pd.ExcelWriter('rndm_search_outputs/all_incidence_iter_100_cv10.xlsx',mode='w') as writer: # check the export name here\n",
    "    to_extract.to_excel(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the best model based on roc auc score\n",
    "\n",
    "best_thresh = \"default\"\n",
    "best_auc = 0.5\n",
    "\n",
    "print(best_thresh)\n",
    "print(best_auc)\n",
    "print(\"Iterations are initiated\")\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "# Testign with auc score\n",
    "for j in range(df.shape[0]):\n",
    "    if df[\"auc_score\"].iloc[j] > best_auc:\n",
    "        print(\"WE FOUND ONE!!!\")\n",
    "        print('Iteration '+ str(j))\n",
    "        best_auc = df[\"auc_score\"].iloc[j]\n",
    "        best_thresh = df[\"threshold_classifier\"].iloc[j]\n",
    "        print(\"Optimum incidence threshold = \",best_thresh)\n",
    "        print(\"Best computed auc = \",best_auc)\n",
    "        print(\"-------------------------------------------------\")\n",
    "        best_index = j\n",
    "\n",
    "        \n",
    "        \n",
    "# print(best_clf)\n",
    "print(\"-----------------------------------\")\n",
    "print(\"END OF ITERATIONS\")\n",
    "print(\"-----------------------------------\")\n",
    "print(df.iloc[best_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Single test runs for optimum threshold & classifier parameters\n",
    "\n",
    "After the optimum threshold and parameter settings are found, this part is used to run the model with ALL  metrics and export it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Receiver Operator Characteristic to assess trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc.sort(reverse=True)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skm.roc_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skm.plot_roc_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the best index from the initial csv\n",
    "\n",
    "best_inc_value = str(df.incidence_threshold.iloc[best_index])\n",
    "print(best_inc_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define x and y for this single test run\n",
    "x =  pd.read_csv(\"data/s-2_features_preprocessed_19_20.csv\") # all features\n",
    "all_thresholds = pd.read_csv(\"data/stress_incidence_labels_25.csv\")  # all 25 labels\n",
    "\n",
    "y = all_thresholds[\"incidence_\" + best_inc_value]\n",
    "\n",
    "# AUGMENT (SMOTE)\n",
    "\n",
    "X_sm, y_sm = sm.fit_resample(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get metrics for the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best performance by RF classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  pd.read_csv(\"data/s-2_features_preprocessed_19_20.csv\") \n",
    "y = all_thresholds.incidence_6\n",
    "\n",
    "sm = SMOTE(random_state=4)\n",
    "\n",
    "X_sm, y_sm = sm.fit_resample(x,y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = skl.train_test_split(X_sm, y_sm, test_size=0.33, random_state=4)\n",
    "\n",
    "clf = rf(n_estimators=200, min_samples_split=2, criterion='entropy', random_state=4)\n",
    "\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split data into training and testing sets\n",
    "clf_type = 'RF'\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = skl.train_test_split(X_sm, y_sm, test_size=0.33, random_state=4)\n",
    "\n",
    "# train an RF model on the training set\n",
    "clf = rf(n_estimators=df.trees[best_index])\n",
    "\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "# apply prediction on the testing set\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = skm.confusion_matrix(y_test,y_pred)\n",
    "print(confusion)\n",
    "\n",
    "TP = confusion[1,1]\n",
    "TN = confusion[0,0]\n",
    "FP = confusion[0,1]\n",
    "FN = confusion[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "skm.plot_confusion_matrix(clf, x_test,y_test, cmap='Greys',ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('D:\\Dropbox\\Publications\\sentinel-2 olive tree stress detection\\Results & Discussion\\\\figures\\\\best_threshold_6_rf.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics for the model\n",
    "\n",
    "accuracy = skm.accuracy_score(y_test,y_pred)\n",
    "avg_precision = skm.average_precision_score(y_test,y_pred)\n",
    "f1_score = skm.f1_score(y_test,y_pred)\n",
    "recall = skm.recall_score(y_test, y_pred)\n",
    "spec = TN / float(TN + FP)\n",
    "fpr = FP / float(TN + FP)\n",
    "tpr = TP / float(TP + FN)\n",
    "roc_auc = skm.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"accuracy = \", accuracy)\n",
    "print(\"average precision = \",avg_precision)\n",
    "print(\"f1 score = \",f1_score)\n",
    "print(\"recall = \",recall)\n",
    "print(\"specificity = \",spec)\n",
    "print(\"false positive rate (1 - specificity) = \",fpr)\n",
    "print(\"true positive rate (sensitivity) = \",tpr)\n",
    "print(\"**area under the ROC curve** = \",roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skm.plot_roc_curve(clf, x_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics for the model\n",
    "\n",
    "accuracy = skm.accuracy_score(y_test,y_pred)\n",
    "avg_precision = skm.average_precision_score(y_test,y_pred)\n",
    "f1_score = skm.f1_score(y_test,y_pred)\n",
    "recall = skm.recall_score(y_test, y_pred)\n",
    "spec = TN / float(TN + FP)\n",
    "fpr = FP / float(TN + FP)\n",
    "tpr = TP / float(TP + FN)\n",
    "roc_auc = skm.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"accuracy = \", accuracy)\n",
    "print(\"average precision = \",avg_precision)\n",
    "print(\"f1 score = \",f1_score)\n",
    "print(\"recall = \",recall)\n",
    "print(\"specificity = \",spec)\n",
    "print(\"false positive rate (1 - specificity) = \",fpr)\n",
    "print(\"true positive rate (sensitivity) = \",tpr)\n",
    "print(\"**area under the ROC curve** = \",roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exports for presenting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the best performing models from each categorie (best threshold/classifier/etc)\n",
    "\n",
    "dict_compare = {\"inc_thres\":\"incidence_\"+best_inc_value,\"classifier_type\":clf_type, \"accuracy\":accuracy,\"averag_precision\":avg_precision,\n",
    "                \"f1_score\":f1_score, \"recall\":recall, \"specificity\":spec,\"false_positive_rate\":fpr,\"true_positive_rate\":tpr,\"roc_auc\":roc_auc\n",
    "               }\n",
    "df_opt_clfs = pd.DataFrame(dict_compare, columns=[\"inc_thres\",\"classifier_type\", \"accuracy\",\"averag_precision\", \"f1_score\", \"recall\",\n",
    "                                                  \"specificity\",\"false_positive_rate\", \"true_positive_rate\", \"roc_auc\", \n",
    "                                                 ], index=[0]\n",
    "                          )"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
